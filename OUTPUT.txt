
> Write a prompt for a research agent that searches the internet based upon the current technologies and choices that have been made 
in the project to find the best techniques or libraries of doing something, and finds the best choice base upon simplicity, 
popularity, good support. It should output its findings in reports/yyyy-mm-dd-topic.md and it should have front matter saying when 
it's appropriate or not to use.  It should explain how to use the library or software, the theory behind it and its basic usage 
philosphy.  The first part of the report will give a short explaination of what you've found, then it should go into detail on how to
 use it (incuding setup and code examples), and finally it should have a caviats section at the bottom saying when its not right.  It
 should include a bibliography of sources it used that can be followed for more information, and the report should be annotated where
 it get information.  Ask me clarifing questions one at time before going writing the research prompt 
  ⎿  ⧉ Selected 1 lines from STEPS.md in Cursor

⏺ I'd be happy to help write a research agent prompt. Let me ask some clarifying questions one at a time.

  Question 1: What is the primary technology stack or programming language for this project? (e.g., TypeScript/Node.js, Python, Go,
  etc.) This will help the research agent understand the context for library recommendations.

> the prompt should examine the current project's technology choices 

⏺ Got it - the agent should introspect the project first to understand the tech stack before researching. That makes it more
  flexible.

  Question 2: Should the research agent focus on a single "best" recommendation, or should it present 2-3 top options with a clear
  recommendation of which one to use and why?

> it should ask clarifying questions until it gets to the best, so make those questions count 

⏺ Understood - the agent should ask targeted, high-value questions to efficiently narrow down to the single best choice rather than
  presenting multiple options.

  Question 3: When the agent annotates sources in the report, what citation style do you prefer? For example:
  - Inline links [source](url)
  - Numbered footnotes [1] with a references section
  - Superscript markers linking to bibliography

> numbered footnotes 

⏺ Good choice - numbered footnotes [1] with a references section at the bottom.

  Question 4: For the front matter that describes when to use/not use the library - should this be in a specific format like YAML
  front matter (common in markdown static sites), or just a plain text section at the top of the document?

> YAML 

⏺ YAML front matter it is.

  Question 5: Should the code examples in the report be minimal snippets showing core usage, or more complete working examples that
  could be copy-pasted into the project with minimal modification?

> the target audience for this is an LLM, so it should be focused on what most effectively conveys the key information 

⏺ That's an important detail - optimizing for LLM consumption means focusing on clear patterns, explicit relationships, and
  unambiguous explanations rather than human-friendly prose.

  Question 6: Should the report include information about common pitfalls or anti-patterns the LLM should avoid when implementing
  with this library? This could help prevent the LLM from generating problematic code.

> correct 

⏺ Got it - include pitfalls and anti-patterns to prevent bad implementations.

  Question 7: When the research agent asks its clarifying questions to narrow down the best choice, should it limit itself to a
  maximum number of questions, or keep asking until it has enough confidence to make a definitive recommendation?

> it should try to get there with 2-3 questions max 

⏺ 2-3 questions max - so the questions need to be highly discriminating to efficiently narrow the field.

  Question 8: Should the reports/ directory be at the project root, or should the agent be flexible about where to place it based on
  project structure (e.g., if there's already a docs/ folder)?

> project root 

⏺ reports/ at project root.

  Question 9: For the topic portion of the filename (yyyy-mm-dd-topic.md), should it be a short slug like state-management or more
  descriptive like react-state-management-library-comparison?

> more descriptive -- just use your best judgement and write the prompt 

⏺ I have enough to write the prompt. Let me read the current STEPS.md file to understand the context, then write the research agent
  prompt.

⏺ Read(STEPS.md)
  ⎿  Read 53 lines

⏺ Now I'll write the research agent prompt:

⏺ Write(.claude/commands/research.md)
  ⎿  Wrote 146 lines to .claude/commands/research.md                                                                          
     # Research Agent: Find the Best Library or Technique
     You are a research agent that helps find the optimal library, tool, or technique for a specific need within this project.
     ## Your Mission
     Find the single best solution based on: **simplicity**, **popularity**, and **good support/maintenance**.
     ## Process
     … +136 lines (ctrl+o to expand)

⏺ Done. I created the research agent prompt at .claude/commands/research.md.

  Key features:

  1. Project introspection - Automatically examines package.json, pyproject.toml, etc. to understand the tech stack
  2. 2-3 targeted questions - Asks discriminating questions that maximally reduce uncertainty
  3. Definitive recommendation - Outputs ONE best choice, not a comparison
  4. LLM-optimized format - Clear structure, explicit patterns, anti-patterns section
  5. Numbered footnotes - Every claim annotated with [1] style references
  6. YAML front matter - Includes use_when and avoid_when arrays for quick scanning

  Usage:
  /research how to do form validation
  /research state management for complex forms
  /research authentication strategy

  The report will be saved to reports/YYYY-MM-DD-descriptive-topic.md with sections for Summary, Philosophy, Setup, Core Usage
  Patterns, Anti-Patterns & Pitfalls, Caveats, and References.